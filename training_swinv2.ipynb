{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQOvOhnKQ-Tu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ftfy regex tqdm matplotlib opencv-python torch scipy scikit-image datasets transformers transformers[torch] accelerate -U\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install tqdm\n",
        "!pip install torchvision tensorboard pillow\n",
        "!pip install evaluate\n",
        "!pip install einops\n",
        "!pip install timm\n",
        "!pip install captum\n",
        "\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from scipy.ndimage import filters\n",
        "from torch import nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weird error that poped up in (import clip) fix patch. Don't run if not necessary\n",
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "id": "n0eMgHiScQDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hW_0GDVVqm7"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiysF11LjNB1"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "train_ds = load_dataset('food101', split=\"train\")\n",
        "validation_ds = load_dataset('food101', split=\"validation\")\n",
        "\n",
        "exclude_idx = []\n",
        "exclude_idx2 = []\n",
        "for x in range(len(train_ds)):\n",
        "  if train_ds[x][\"image\"].mode != 'RGB':\n",
        "    exclude_idx.append(x)\n",
        "\n",
        "for x in range(len(validation_ds)):\n",
        "  if validation_ds[x][\"image\"].mode != 'RGB':\n",
        "    exclude_idx2.append(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create new dataset exluding those idx\n",
        "train_ds_new = train_ds.select(\n",
        "    (\n",
        "        i for i in range(len(train_ds))\n",
        "        if i not in set(exclude_idx)\n",
        "    )\n",
        ")\n",
        "\n",
        "validation_ds_new = validation_ds.select(\n",
        "    (\n",
        "        i for i in range(len(validation_ds))\n",
        "        if i not in set(exclude_idx2)\n",
        "    )\n",
        ")\n",
        "\n",
        "ds = datasets.DatasetDict({\"train\":train_ds_new,\"validation\":validation_ds_new})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nobvAXaLNZct",
        "outputId": "42619d7e-f141-4306-de08-2cbcc875ae66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parameter 'indices'=<generator object <genexpr> at 0x781c894ee420> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "WARNING:datasets.fingerprint:Parameter 'indices'=<generator object <genexpr> at 0x781c894ee420> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vRlVtIqGwSa"
      },
      "source": [
        "# ViT Implementation & Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Swinv2ForImageClassification, AutoImageProcessor, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, load_metric\n",
        "import torch\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
        "\n",
        "train_dataset = train_ds_new\n",
        "eval_dataset = validation_ds_new\n",
        "\n",
        "def process_example(example_batch):\n",
        "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
        "    inputs['label'] = example_batch['label']\n",
        "    return inputs\n",
        "\n",
        "def transform(example_batch):\n",
        "    # Taking a list of PIL images and turn them to pixel values\n",
        "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
        "    # Including the labels\n",
        "    inputs['label'] = example_batch['label']\n",
        "    return inputs\n",
        "\n",
        "prepared_ds = ds.with_transform(transform)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['label'] for x in batch])\n",
        "    }\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
        "\n",
        "labels = ds['train'].features['label'].names\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "# model_vit = AutoModel.from_pretrained(\"/content/drive/MyDrive/vit-food-swinv2/vit-food-v1/checkpoint-142050/\")\n",
        "\n",
        "model_vit = AutoModelForImageClassification.from_pretrained(\n",
        "    \"/content/drive/MyDrive/vit-food-swinv2/vit-food-v1/checkpoint-142050/\",\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")\n",
        "\n",
        "feature_extractor = AutoImageProcessor.from_pretrained(\n",
        "    \"/content/drive/MyDrive/vit-food-swinv2/vit-food-v1/checkpoint-142050/\",\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-food-swinv2\",\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  num_train_epochs=15,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_strategy=\"epoch\",\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_vit,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_ds[\"train\"],\n",
        "    eval_dataset=prepared_ds[\"validation\"],\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "train_results=trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "metrics = trainer.evaluate(prepared_ds['validation'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "# ViT Predictions\n",
        "predictions = trainer.predict(prepared_ds['validation'])\n",
        "vit_predictions = predictions.predictions.argmax(-1)"
      ],
      "metadata": {
        "id": "smKVtWVq2bdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpV2BpM4D_LR"
      },
      "outputs": [],
      "source": [
        "!cp -r vit-food-v1/ ../content/drive/MyDrive/vit-food-swinv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r vit-food-swinv2.zip vit-food-v1\n",
        "from google.colab import files\n",
        "files.download('vit-food-swinv2.zip')"
      ],
      "metadata": {
        "id": "3hLfBCMG0NId"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}