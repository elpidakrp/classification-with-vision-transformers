{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQOvOhnKQ-Tu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ftfy regex tqdm matplotlib opencv-python torch scipy scikit-image datasets transformers transformers[torch] accelerate -U\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install tqdm\n",
        "!pip install torchvision tensorboard pillow\n",
        "!pip install evaluate\n",
        "!pip install einops\n",
        "!pip install timm\n",
        "!pip install captum\n",
        "\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from scipy.ndimage import filters\n",
        "from torch import nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weird error that poped up in (import clip) fix patch. Don't run if not necessary\n",
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "id": "HGQCVo0hcOrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hW_0GDVVqm7"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiysF11LjNB1"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "train_ds = load_dataset('food101', split=\"train\")\n",
        "validation_ds = load_dataset('food101', split=\"validation\")\n",
        "\n",
        "exclude_idx = []\n",
        "exclude_idx2 = []\n",
        "for x in range(len(train_ds)):\n",
        "  if train_ds[x][\"image\"].mode != 'RGB':\n",
        "    exclude_idx.append(x)\n",
        "\n",
        "for x in range(len(validation_ds)):\n",
        "  if validation_ds[x][\"image\"].mode != 'RGB':\n",
        "    exclude_idx2.append(x)\n",
        "\n",
        "# create new dataset exluding those idx\n",
        "train_ds_new = train_ds.select(\n",
        "    (\n",
        "        i for i in range(len(train_ds))\n",
        "        if i not in set(exclude_idx)\n",
        "    )\n",
        ")\n",
        "\n",
        "validation_ds_new = validation_ds.select(\n",
        "    (\n",
        "        i for i in range(len(validation_ds))\n",
        "        if i not in set(exclude_idx2)\n",
        "    )\n",
        ")\n",
        "\n",
        "ds = datasets.DatasetDict({\"train\":train_ds_new,\"validation\":validation_ds_new})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vRlVtIqGwSa"
      },
      "source": [
        "# ViT Implementation & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lntY_8A3xgpb"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from transformers import TrainingArguments\n",
        "from transformers import ViTForImageClassification\n",
        "from transformers import Trainer\n",
        "\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "# model_name_or_path = 'google/vit-large-patch16-224'\n",
        "processor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "\n",
        "def process_example(example_batch):\n",
        "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
        "    inputs['label'] = example_batch['label']\n",
        "    return inputs\n",
        "\n",
        "def transform(example_batch):\n",
        "    # Taking a list of PIL images and turn them to pixel values\n",
        "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
        "    # Including the labels\n",
        "    inputs['label'] = example_batch['label']\n",
        "    return inputs\n",
        "\n",
        "prepared_ds = ds.with_transform(transform)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['label'] for x in batch])\n",
        "    }\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
        "\n",
        "labels = ds['train'].features['label'].names\n",
        "\n",
        "model_vit = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-food-v2\",\n",
        "  per_device_train_batch_size=64,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  num_train_epochs=10,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_strategy=\"epoch\",\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_vit,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_ds[\"train\"],\n",
        "    eval_dataset=prepared_ds[\"validation\"],\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "# from transformers import AutoModel, Trainer\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"/content/vit-food-v1/checkpoint-9470\")\n",
        "# train_results = trainer.train(resume_from_checkpoint=\"/content/vit-food-v1/checkpoint-9470\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_vit.to(device)\n",
        "\n",
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "metrics = trainer.evaluate(prepared_ds['validation'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "# ViT Predictions\n",
        "predictions = trainer.predict(prepared_ds['validation'])\n",
        "vit_predictions = predictions.predictions.argmax(-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r vit-food-v2/ ../content/drive/MyDrive"
      ],
      "metadata": {
        "id": "2Fh96b1iUbvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV8PU-PoCk1s"
      },
      "outputs": [],
      "source": [
        "!zip -r vit-food-v1.zip vit-food-v1\n",
        "from google.colab import files\n",
        "files.download('vit-food-v2.zip')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}